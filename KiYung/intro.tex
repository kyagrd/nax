\chapter{Introduction}\label{ch:intro}

In this dissertation, we contribute to answering the question:
``how does one build a seamless system where programmers can both
write (functional) programs and formally reason about those programs''.
We set the scope of our research by clarifying what we mean by
\emph{programming} and \emph{formal reasoning}, and how they are related
(\S\ref{sec:intro:scope}). We motivate our research by describing the gap
between the designs of functional programming languages and
formal reasoning systems (\S\ref{sec:intro:motiv}), and assert
our thesis (\S\ref{sec:intro:motiv}).

TODO

preliminary concepts (\S\ref{sec:intro:concepts})

contributions (\S\ref{sec:intro:contrib})

methodology and overview (\S\ref{sec:intro:overview})

TODO

\section{Programming and Formal Reasoning}\label{sec:intro:scope}

In our research, \emph{programming} refers to writing programs in
modern functional programming languages (\eg, Haskell, ML) that support
(recursive) datatypes, higher-order functions, (parametric) polymorphism,
and type inference. When we say \emph{functional programming languages},
\emph{functional languages}, or \emph{programming languages},
we mean such modern functional programming languages. We do not consider,
so called, dynamically typed functional languages (\eg, Lisp, Erlang)
in this dissertation.

\emph{Formal reasoning}, or \emph{logical reasoning},
refers to constructing proofs with dependently-typed formal reasoning systems
(\eg, Coq, Agda) that support (inductive) datatypes, higher-order functions,
polymorphism (\ie, type arguments to type constructors), and, of course,
dependent types (\ie, term arguments to type constructors). When we say
\emph{formal reasoning systems}, \emph{logical reasoning systems},
or, simply \emph{reasoning systems}, we mean such dependently-typed
formal reasoning systems.

Functional languages and reasoning systems are closely related.
Proofs in reasoning systems are similar in structure to programs
in functional languages. For example, the Haskell program \textit{id},
which computes a value of type $A$ when given a value of type $A$,
and, the Agda proof \textit{id}, which proves that the proposition $A$
is true when given a proof of $A$, are very similar:
\begin{center}
\begin{singlespace}
\begin{minipage}{.4\linewidth}
	\textsc{Haskell} \vspace*{.5em} \\
\textit{id} $::$ $A -> A$ \\
\textit{id} = $\lambda x -> x$
\end{minipage}
\begin{minipage}{.4\linewidth}
	\textsc{Agda}  \vspace*{.5em} \\
\textit{id} : $A -> A$ \\
\textit{id} = $\lambda x -> x$
\end{minipage}
\end{singlespace}
\end{center}\vspace*{.5em}
Such similarities are not accidental but intended by the design of
reasoning systems, based on the observation that proofs corresponds
to programs and propositions corresponds to types. This observation
is known as the Curry--Howard correspondence \cite{Howard69}.
The \emph{reasoning systems} we consider in this dissertation
are the ones that are based on the Curry--Howard correspondence.
We will explain some preliminary concepts related to correspondence
in the following subsections.

We assume that readers are familiar with \emph{programming}
in \emph{functional languages} and some basic concepts of lambda calculus
(\eg, $\beta$-reduction, normal forms). We assume that the readers have
basic knowledge on logic (\eg, concepts of axioms, inference, and proofs)
but not necessary familiar with \emph{formal reasoning}
in \emph{reasoning systems}.

\subsection{The Curry--Howard correspondence}
In late 1960s, \citet{Howard69} observed that intuitionistic natural deduction,
which is a proof system of a formal logic, and a typed lambda calculus,
which is a model of computation, are directly related. This relation,
known as the Curry--Howard correspondence, is established as follows:
\begin{itemize}
\item A proposition in the natural deduction corresponds to
	a type in the lambda calculus.
\item A proof for the proposition corresponds to
	a term of the type, to which that proposition corresponds.
\item Simplification of proofs correspond to computation,
	that is, simplification of terms.
\end{itemize}
We explain this correspondence with a very simple
version of intuitionistic natural deduction and
a simply-typed lambda calculus (Figure\;\ref{fig:nd}).\footnote{
	The readers who are familiar to the literature on natural deduction
	would notice that the left column in Figure\;\ref{fig:nd} is not
	the same style as the natural deduction formalized by
	\citet{Gentzen35,Gentzen69}. It has a has the flavours of
	both natural deduction and the sequent calculus. The context $\Gamma$
	are part of the syntax of judgments as in sequent calculus,
	but rather than the very syntactic structural rules (weakening,
	contraction, permutation) of sequent calculus, we simply rely on
	$A^x \in \Gamma$ to use the hypothetical propositions from $\Gamma$.
	We choose to formalize natural deduction this way to emphasize
	the structural similarities to the typical formalization
	for typing rules of lambda calculi.}

Traditionally, in Hilbert-style deduction, a formal logic is formalized
by a minimal set of inference rules (\eg, modus ponens
$\inference{\Gamma |- A -> B & \Gamma |- A}{\Gamma |- B}$)
and a set of axiom schemes (\eg, $A -> A$ and $A -> (B -> A)$ where
the meta-variables $A$ and $B$ can be instantiated to arbitrary propositions).
\emph{Natural deduction}, on the other hand, is another style of
formalizing logic mostly by a set of inference rules and a minimal
(often empty) set of axiom schemes.

There are many variations of formal logic based on natural deduction
(and also their corresponding typed lambda calculi) depending on
the set of logical connectives, constants, and quantifiers they support.
We illustrate a intuitionistic natural deduction with implication ($->$) and
falsity ($\bot$), and, its corresponding simply-typed lambda calculus (STLC)
in Figure\;\ref{fig:nd}. The implication and the falsity in natural deduction
corresponds to the function type ($->$) and the void type ($\bot$) in the STLC.
In the inference rules (\rulename{Ax}) and (\rulename{$->_\textsc{I}$}),
$x$ on $A^x$ is a meta-tag to distinguish between possibly multiple
occurrences of $A$ in $\Gamma$ (\eg, $\Gamma = A^{x_1},B^{x_2},A^{x_3}$).

\begin{figure}
\[
\begin{array}{ccc}
	\text{Intuitionistic natural deduction} & &
	\text{Typing rules of STLC} \\ \\
\inference{A^x \in \Gamma}{\Gamma |- A} & (\textsc{Ax}) &
\inference{x:A \in \Gamma}{\Gamma |- A} \\ \\
\inference{\Gamma,A^x |- B}{\Gamma |- A -> B} & (->_\textsc{I}) &
\inference{\Gamma,x:A |- B}{\Gamma |- \l x.t : A -> B} \\ \\
\inference{\Gamma |- A -> B & \Gamma |- A}{\Gamma |- B} & (->_\textsc{E}) &
\inference{\Gamma |- t : A -> B & \Gamma |- s : A}{\Gamma |- t\;s : B} \\ \\
\inference{\Gamma |- \bot}{\Gamma |- A} & (\bot_\textsc{E})  &
\inference{\Gamma |- t:\bot}{\Gamma |- \texttt{elim}_\bot\,t : A}
\end{array}
\]
\caption{Intuitionistic natural deduction with implication and falsity,
	and its corresponding simply-typed lambda calculus.}
\label{fig:nd}
\end{figure}

Note that each typing rule (in the right column) has exactly the same structure
as its corresponding inference rule (in the left column), except for
the variables and terms appearing on the right-hand side of the colon
(\eg, $x$ in $x:A$ and $t$ in $t:A -> B$). So, a type correct term,
which is justified by the derivation following the typing rules of
the lambda calculus, captures the structure of its corresponding proof
by natural deduction. For instance, $(\l x. x)$ is a term that captures
the structure of a proof for $A -> A$.
For this reason, such type correct terms in reasoning systems
are called proof terms, proof objects, or simply proofs.

We explained that proofs and propositions in the natural deduction
corresponds to terms and types in the lambda calculus. Lastly,
we need to show the correspondence between simplification of proofs
and simplification of terms (\ie, computation) in order to establish
the Curry--Howard correspondence between the natural deduction
and the lambda calculus.

There can be multiple proofs for the same proposition, and,
correspondingly, there can be more than one term of the same type.
Some of these terms are closely related while others are rather independent.
For example, $\l x_1.\l x_2.x_1$ and $\l x_1.\l x_2.x_2$ are rather 
independent terms that inhabit the same type $A -> A -> A$.
The typing derivations for these terms (right column) and
its corresponding proofs (left column) are illustrated in
Figure\;\ref{fig:proofAAA}.
\begin{figure}
\[\setpremisesend{.1em}
\inference[$(->_\textsc{I})^{x_1}$]
	{\hspace*{-3em}
	 \inference[$(->_\textsc{I})^{x_2}$]
	 	{\hspace*{-3em}
		 \inference[$(\textsc{Ax})^{x_1}$]
			{A^{x_1}\in A^{x_1},A^{x_2}}
			{A^{x_1},A^{x_2} |- A} }
		{A^{x_1} |- A -> A} }
	{|- A -> A -> A }
\qquad
\inference[($->_\textsc{I}$)]
	{\hspace*{-1.3em}
	 \inference[($->_\textsc{I}$)]
		{\hspace*{-1.5em}
		 \inference[(\textsc{Ax})]
			{x_1 : A \in x_1 : A, x_2 : A}
			{x_1 : A, x_2 : A |- x_1 : A} }
		{x_1 : A |- \l x_2.x_1 : A -> A} }
	{|- \l x_1.\l x_2.x_1 : A -> A -> A }
\]
\[\setpremisesend{.1em}
\inference[$(->_\textsc{I})^{x_1}$]
	{\hspace*{-3em}
	 \inference[$(->_\textsc{I})^{x_2}$]
		{\hspace*{-3em}
		 \inference[$(\textsc{Ax})^{x_2}$]
			{A^{x_2}\in A^{x_1},A^{x_2}}
			{A^{x_1},A^{x_2} |- A} }
		{A^{x_1} |- A -> A} }
	{|- A -> A -> A }
\qquad
\inference[($->_\textsc{I}$)]
	{\hspace*{-1.3em}
	 \inference[($->_\textsc{I}$)]
		{\hspace*{-1.5em}
		 \inference[(\textsc{Ax})]
			{x_2 : A \in x_1 : A, x_2 : A}
			{x_1 : A, x_2 : A |- x_2 : A} }
		{x_1 : A |- \l x_2.x_2 : A -> A} }
	{|- \l x_1.\l x_2.x_2 : A -> A -> A }
\]
\caption{Typing derivations (right) for terms of type $A -> A -> A$
	and their corresponding proofs (left).}
\label{fig:proofAAA}
\end{figure}

On the other hand, there are closely related terms of the same type.
For example, $(\l x_1.x_2) x_3$ reduces to $x_2$ by a $\beta$-reduction step
(\ie, $(\l x_1.x_2) x_3 -->_\beta x_2$). A term like $x_2$, which does not
($\beta$-)reduce any further, is called a ($\beta$-)\emph{normal form} or
a ($\beta$-)\emph{normal term}. \citet{Pra65} established a notion of
reduction and normalization for natural deduction. One can reduce a proof
when there is a consecutive use of introduction and elimination rules.
An introduction rule introduces a certain form of propositions
in the conclusion (below the horizontal bar), and, an elimination rule
uses propositions that form in the premises (above the horizontal bar).
The rules (\rulename{$->_\textsc{I}$}) and (\rulename{$->_\textsc{E}$})
are introduction and elimination rules for implication ($->$).
So, we need to show the correspondence between reduction of proofs
over implication in the natural deduction and ($\beta$-reduction) over
type correct terms in the lambda calculus,
in order to establish the Curry--Howard correspondence.

Figure\;\ref{fig:proofreduce} illustrates that
the reduction of a proof involving consecutive uses of
(\rulename{$->_\textsc{I}$}) and (\rulename{$->_\textsc{E}$})
corresponds to the $\beta$-reduction of well-typed terms.\footnote{
	Introduction and elimination rules for ($->$) consecutively used
	in the other order corresponds to $\eta$-reduction
	(\ie, $(\l x. t\;x) -->_\eta t$ where $x$ does not appear free in $t$).
	In this dissertation, we only consider $\beta$-reduction.}
To redcuce the proof, we replace the uses of $A^x$ in the premise of
$(->_\textsc{I})^x$ with the derivations $D'$, which deduces $A$ from
the original context $\Gamma$, so that we can remove $A^x$ from
the left-hand sides of the turstile ($|-$) throughout the proof.
The change of subscripts from $\mathcal{D}_{[\Gamma,A^x]}$ to
$\mathcal{D}_{[\Gamma]}$, before and after the reduction,
denotes that we consistently removed $A^x$ from the context
(\ie, left-hand sides of $|-$). We leave it as an exercise for
the readers to construct a corresponding proof for the reduction
$(\l x_1.x_2) x_3 -->_\beta x_2$ from the previous paragraph
(\textit{hints}: Start with $\Gamma = x_2:A,x_3:B$).

\begin{figure}
Reducing a proof involving consecutive uses of
(\rulename{$->_\textsc{I}$}) and (\rulename{$->_\textsc{E}$}):
\[
\inference[($->_\textsc{E}$)]
	{\hspace*{-3em}
	 \inference[$(->_\textsc{I})^{x}$]
		{\hspace*{-1em}
		 \inference*
			{\hspace*{-4em}
			 \inference[$(\textsc{Ax})^{x}$]
				{A^x\in\Gamma,A^x,\Gamma'}
				{\Gamma,A^x,\Gamma' |- A} \\
			 \vdots \\
			 \mathcal{D}_{[\Gamma,A^x]} }
			{\Gamma,A^x |- B} }
		{\Gamma |- A -> B}
	&\inference*{\mathcal{D}'_{[\Gamma]}}{\Gamma |- A} }
	{\Gamma |- B}
\quad
\longrightarrow
~
\inference*
	{\inference*
		{\mathcal{D}'_{[\Gamma,\Gamma']}}
		{\Gamma,\Gamma' |- A} \\
	 \vdots_{\phantom{G}} \\
	 \mathcal{D}_{[\Gamma]} }
	{\Gamma |- B}
\]
Reduction of a well-typed term along with its typing derivation:
\[
\inference[($->_\textsc{E}$)]
	{\hspace*{-3em}
	 \inference[$(->_\textsc{I})$]
		{\hspace*{-1em}
		 \inference*
			{\hspace*{-4em}
			 \inference[$(\textsc{Ax})$]
				{x:A\in\Gamma,x:A,\Gamma'}
				{\Gamma,x:A,\Gamma' |- x:A} \\
			 \vdots \\
			 \mathcal{D}_{[\Gamma,x:A]} }
			{\Gamma,x:A |- t:B} }
		{\Gamma |- \l x.t : A -> B}
	&\inference*{\mathcal{D}'_{[\Gamma]}}{\Gamma |- s:A} }
	{\Gamma |- (\l x.t)\,s:B}
\quad
\longrightarrow_\beta
~
\inference*
	{\inference*
		{\mathcal{D}'_{[\Gamma,\Gamma']}}
		{\Gamma,\Gamma' |- s:A} \\
	 \vdots_{\phantom{G}} \\
	 \mathcal{D}_{[\Gamma]} }
	{\Gamma |- t[s/x]:B}
\]
\caption{Reduction of a proof involving introduction and elimination rules
	for implication, and its corresponding $\beta$-reduction 
	of a well-typed term.}
\label{fig:proofreduce}
\end{figure}

We illustrated the Curry--Howard correspondence between
a minimal intuitionistic logic and its corresponding lambda calculus
In Figure\;\ref{fig:nd}. That is, a proposition, its proof, and
simplification of proofs (or, proof reduction) corresponds to
a term, its type, and simplification of terms (or, computation).
The first and second pieces of the corresponds, a proposition and
its proof corresponding to a term and its type, is quite obvious from
the structural similarity between the inference rules and the typing rules.
The last piece, correspondence between proof reduction and computation,
needs further analysis on the implication and elimination rules.
We illustrated the correspondence between proof reduction over implication
and $\beta$-reduction, which describes the computation over functions,
in Figure \ref{fig:proofreduce}. The correspondence between
the proof reduction over falsity and the computation over the void type
hold vacuously. There is no proof reduction and computation over $\bot$
since it lacks the introduction rule -- it only has the elimination rule
(\rulename{$\bot_\textsc{E}$}).

\section{Logical consistency and strong normalization}
A logic is \emph{consistent} when not all propositions are provable.
This property, \emph{logical consistency}, is absolutely necessary for
a logic to be meaningful, that is, to be able to justify true propositions
and refute false propositions. A standard way to show logical consistency
is to find a sound model\footnote{In this dissertation, we mean \emph{logic}
	in a proof theoretic sense. A model is sound with respect to logic
	described by a proof system when any provable proposition
	in the logic is interpreted as a truth value in the model.}
for the logic, and show that there exist a proposition whose interpretation
in the model is a falsity value.

For instance, to show that the logic described in Figure\;\ref{fig:nd}
is consistent, we would construct a sound model such that the meaning of
$\bot$ is a falsity value. Using the Curry--Howard correspondence,
we can construct a model for the logic using the syntactic structure of its
corresponding lambda calculus. We define the interpretation $[|A|]$ for
the proposition $A$ as the set of terms of type $A$ in the lambda calculus.
Then, we can show that there exist no closed terms in $[|\bot|]$,
which implies that $|- \bot$ cannot be proved. In Chapter\;\ref{ch:poly},
we construct models for several lambda calculi based on this idea of
interpreting propositions (or, types) as set of well-typed terms.
Since our scope of research is for reasoning systems, which are based on
the Curry--Howard correspondence, we will describe mostly in terms of
lambda calculi, without mentioning their corresponding natural deductions,
from now on. For instance, we will consider that these models are for
lambda calculi, rather than for natural deductions.

Construction of such models typically assume strong normalization of
the lambda calculus.\footnote{Strong normalization is a property that
all well-typed terms reduce to their normal form regardless of
the reduction strategy (\ie, choice of which redex to reduce first).}
In the models we construct in Chapter\;\ref{ch:poly},
the interpretation $[| A |]$ for the type $A$ is inductively defined as follows:
\begin{itemize}
	\item Base case: normal forms of type $A$ are in $[| A |]$
	\item Inductive case: if $t' \in [| A |]$ and $t -->_\beta t'$
		then $t \in [| A |]$.
\end{itemize}
That is, interpretations of types are equivalence classes of their
well-typed normal forms.
We show that each of the calculi in Chapter\;\ref{ch:poly} are
logically consistent by showing that there is no closed term
in the interpretation of the void type
(\ie, $\iota$ in the STLC (\S\ref{sec:stlc}) and $\forall a.a$ in
the polymorphic lambda calculi (\S\ref{sec:f},\S\ref{sec:fw})).

When we admit non-terminating (or, diverging) terms in a lambda calculus,
the definition for interpretations of types would become more complicated,
and we could not establish the Curry--Howard correspondence because those
non-terminating terms cannot be considered as proofs in general. In fact,
it is well-known that functional languages are logically inconsistent,
if we try to view diverging terms (\ie, non-terminating programs)
in functional languages as proofs by a naive Curry--Howard correspondence.
For example, the non-terminating program $\textit{loop} = \textit{loop}$
in Haskell can inhabit arbitrary type (even $\forall a.a$,
which is the polymorphic encoding of the void type). Intuitively, such
non-terminating programs correspond to logical fallacies of
circular reasoning (\ie, arguing something by assuming the same thing)
to view programs like \textit{loop} as proofs. For this reason,
reasoning systems are designed to be strongly normalizing, unlike
functional languages. We also limit the scope of our research to
strongly normalizing languages since one of the design goals for our language
system is logically consistency under the Curry--Howard correspondence.

%% \footnote{
%% 	There are some recent studies (\eg, \cite{BauSto09})
%% 	on program extraction, which view \emph{realizability} \cite{Kle45}
%% 	as a concept that subsumes the Curry--Howard correspondence.
%% 	But they are for logical reasoning to construct proofs but
%% 	for more flexible extraction of programs, which may involve
%% 	non-termination and possibly other computational effects.}

\section{Datatypes and recursion schemes}
\label{ssec:intro:scope:datarec}
\begin{figure}
\begin{singlespace}
\centering
\begin{tabular}{p{.43\linewidth}|p{.44\linewidth}}
Datatypes in functional languages
&
Datatypes in reasoning systems
\\ \hline \hline
Datatypes may involve diverging computations         
(\eg, functions defined by general recursion)
&
The Curry--Howard correspondence must holds for all datatypes
\\ \hline
Type constructors may have type arguments
&
Type constructors may have term arguments (or, term indices)
as well as type arguments
\end{tabular}
\end{singlespace}
\caption{Comparison of datatypes in functional languages
	and datatypes reasoning systems}
\label{fig:datadiff}
\end{figure}
Both functional languages and reasoning systems support many other language
constructs than the lambda calculus we discussed in Figure\;\ref{fig:nd}.
Among those constructs, datatypes and recursion are most common and essential.
Datatype definitions in both programming languages and reasoning systems have
the form of disjoint sums (over several data constructors) of products
(of the argument types for each data constructor). For example, in Haskell,
we can define a datatype (\textit{Diagram}) that defines a diagram,
which is ether empty, a point (\textit{Point}) defined by a single coordinate,
a line segment (\textit{LineSeg}) defined by two coordinates, or
a triangle (\textit{Triangle}) defined by three coordinates,
as follows:\footnote{
	We can also define \textit{Coord} as a non-recursive datatype
	and understand it as a product of numbers, but here, you can just
	consider \textit{Coord} given as primitive type for simplicity.}
\vspace*{-2em}
\begin{singlespace}
\begin{verbatim}
  data Diagram = Empty
               | Point Coord
               | LineSeg Coord Coord
               | Triangle Coord Coord Coord
\end{verbatim}
\end{singlespace}~\vspace*{-3em}\\
In type theories, we can understand the datatypes above as
\[ \textit{Diagram} ~ \triangleq ~
	\textit{Unit} + \textit{Coord} + (\textit{Coord} \times \textit{Coord})
	+ (\textit{Coord} \times \textit{Coord} \times \textit{Coord}) \]
where $+$ and $\times$ are binary operators for sums and products
and \textit{Unit} is the identity for products (\ie, 0-tuple).
Non-recursive datatypes in reasoning systems can be understood in the same way,
disregarding dependent types (\ie, types indexed by terms). It is well-known
that the Curry--Howard correspondence between the type operators, sums ($+$)
and products ($\times$), and the logical connectives, disjunction ($\lor$) and
conjunction ($\land$).

We summarized the different characteristics between datatypes
in functional languages and datatypes in reasoning systems
in Figure\;\ref{fig:datadiff}. Our approach on datatypes
in this dissertation is to find a common middle ground
balancing between the desired properties of functional languages
(few restrictions on datatype definitions) and reasoning systems
(logical consistency, term indices). When we disregard general recursion
from functional languages and disregard dependent types from reasoning systems,
non-recursive datatypes in functional languages and reasoning systems coincide. However, for datatypes that are defined in terms of themselves, the situation
is more subtle -- they do not coincide even when we disregard general recursion
and dependent types.

Datatypes defined in terms of themselves are called \emph{recursive datatypes}
in functional languages and \emph{inductive datatypes} in reasoning systems.
Recursive datatypes in functional languages have few restrictions.
Any syntactically valid\footnote{
	More accurately, what we really mean is \emph{well-kinded} rather than
	syntactically valid, if you are familiar to kinds in type systems.
	For who are not, you will see them in later chapters, such as
	in \S\ref{sec:fw} where we describe System \Fw.}
datatype definitions are admitted as valid types. On the other hand,
Inductive datatypes in reasoning systems have additional restrictions so that
only those datatypes, for which the Curry--Howard correspondence hold, are
admitted. Some recursive datatypes, admitted as valid in functional languages,
are not admitted as valid inductive datatypes in reasoning systems. For example,
the Haskell datatype $T$ below would not be admitted in reasoning systems.
\begin{singlespace}
\begin{align*}
& \textbf{data}~T\;a = C\;(T -> a) \quad
          \texttt{-}\texttt{-}~\text{\small datatype recursive on
	  				a contravariant position} \\
& w \,::\, T\;a -> a ~~ \quad
          \texttt{-}\texttt{-}~\text{\small an encoding of the untyped
                                     $(\lambda x.x\;x)$
                                     in a typed language}~ \\
& w = \lambda x -> \,\textbf{case}~x~\textbf{of}~C\;f -> f\;x \\
& \textbf{fix} \,::\, (a -> a) -> a \quad
          \texttt{-}\texttt{-}~\text{\small an encoding of 
                                     $(\lambda f.(\lambda x.f(x\;x))\,
                                                 (\lambda x.f(x\;x)))$} \\
& \textbf{fix} = \lambda f -> (\lambda x -> f (w\;x))\,(C(\lambda x -> f (w\;x)))
\end{align*}
\end{singlespace}\noindent
Surprisingly (if you hadn't known), we can encode the well known
general recursive combinator (\textbf{fix}, \aka\ the \textsf{Y}-combinator)
using a datatype recursive on a contravariant position (\ie, left-hand side of
$->$), without using any recursion at term level.
Datatypes that are recursive only over covariant\footnote{Type arguments
	without $->$ are by default in covariant positions. Right-hand sides
	of $->$ are covariant, and, a contravariant position of
	a contravariant position (\eg, $A$ in $(A -> B) -> B$) is
	also covariant.}
positions are called \emph{positive datatypes}, and, datatypes
that are recursive over one or more contravariant positions are
called \emph{negative datatypes}. Negative datatypes are not admitted
in reasoning systems since they might cause non-termination. Recall that
the Curry--Howard correspondence is establish when proofs are normalizing.

\paragraph{}
Each inductive datatype admitted in reasoning systems must come with
a recursion scheme (or, induction principle),\footnote{
	Induction principles provided in reasoning systems are
	dependently typed. Disregarding dependent types, these
	induction principles are computationally equivalent to
	recursion schemes such as primitive recursion,
	course-of-values recursion, or lexicographic recursion.
	Since we do not consider dependent types, we will only discuss
	those non-dependently typed recursion schemes.}
whose reduction step follows from the Curry--Howard correspondence
over that datatype, just as the $\beta$-reduction follows from
the Curry--Howard correspondence over functions. The design process
of reasoning systems can be summarized as follows: 
start from a strongly normalizing and logically consistent calculus,
add language extensions, and then theoretically justify that those extensions
do not break normalization and consistency. Since datatypes are one of the most
common and significant extensions, it adds a significant theoretical burden to
ensure normalization and logical consistency for datatypes and their recursion
schemes. To ensure normalization and logical consistency, one should construct
a model including interpretations for datatypes and their recursion schemes,
which is more complex than the model for the lambda calculus without datatypes.

There are two ways to ensure normalization for datatypes and their recursion
schemes. One is to restrict datatype definitions (\ie, formation rules) and
the other is to restrict the use of datatypes (\ie, elimination rules).

The former approach, also known as the \emph{conventional} approach,
is taken by reasoning systems (\eg, Coq, Agda) and the studies on
terminating recursion schemes in functional languages following
the Squiggol school of constructive programming \cite{AoP,hagino87phd,Bir87}.
The conventional approach restricts the definition of datatypes and
the key mechanism to check termination of recursion schemes are based on
size decreasing arguments in an untyped setting. Due to the positivity
restriction put on the datatype definitions, one can assume that
recursive values contained inside a data constructor is smaller than
the value which contains them. This assumption of measuring size by
structural containment does not always hold for negative datatypes.

The latter non-conventional approach, known as the \emph{Mendler-style} approach
puts no restrictions on the definition of datatypes, but instead, carefully
restricts the use of, in particular, the decomposition (\ie, elimination, or,
pattern matching) of recursive values. In this approach, theoretical
development for termination of recursion schemes is type based.
Instead of treating datatypes as primitive language constructs, datatypes
and their recursion schemes are embedded into a typed lambda calculus,
which is proven to be strongly normalizing and logically consistent.
Then, there is no need for extra mechanism, other than type checking,
to ensure termination of the recursion schemes. We take this non-conventional
approach in designing our language system. More details on the conventional
approach and the Mendler-style approach is to be discussed
in \S\ref{sec:intro:concepts} and in Chapter\;\ref{ch:mendler}.

\section{Motivation}\label{sec:intro:motiv}
Since the observation of the Curry--Howard correspondence, logicians and
programming language researchers have dreamed of building a system in which
one can both write programs (\ie, model computation) and formally reason about
(\ie, construct proofs of) the properties (\ie, types) of those programs.

However, building a practical system that unifies programming and
formal reasoning, based on the Curry--Howard correspondence, is still
an open research problem. The gap between the conflicting design goals of
typed functional programming languages, such as ML and Haskell,
and formal reasoning systems, such as Coq and Agda, is still wide.
For instance, we discussed in the previous section that datatypes admitted
in functional languages and in reasoning systems do not completely overlap.

\begin{itemize}

\item
Programming languages are designed to achieve computational expressiveness.
They often sacrifice logical consistency for computational expressiveness.
Programmers should be able to conveniently express all possible computations,
regardless of whether those computations have a logical interpretation
(by the Curry--Howard correspondence) or not.

\item
Formal reasoning systems are designed to achieve logical consistency.
They often sacrifice computational expressiveness for logical consistency.
Users expect that it is only possible to prove true propositions,
and it is impossible to prove falsity. They are willing to live with
the difficultly (or even inability) to express certain computations
within the reasoning system in order to achieve logical consistency.

\end{itemize}

As a result, the recursion schemes of programming languages and
formal reasoning systems differ considerably.
Programming languages provide unrestricted general recursion
to conveniently express computations that may or may not terminate.
Formal reasoning systems provide induction principles for sound reasoning,
or, in the computational view, principled recursion schemes that can only
express terminating computation.

The two different design goals also lead to significant differences
in their type system design as well. Programming languages are based on
\emph{recursive types}, which place few restrictions on the definition of
new datatypes. Programmers can express computations over a wide variety of
datatypes. In addition, most of the functional programing languages have
clear distinction between terms and types (\ie, terms do not appear in types).
Reasoning systems are based on \emph{inductive types}, which place additional
restrictions, accepting only positive datatypes. In addition, terms can appear
in types to specify fine grained properties involving values at term level
(\eg, size invariants of data structures).

This dissertation explores a sweet spot where one can benefit from
the advantages of both programming languages and formal reasoning systems.
That is, we design a unified language system, called Nax, which is
logically consistent while being able to conveniently express
many useful computations. We do this by placing few restrictions
on type definitions, as is done in programming languages, but also
provide a rich set of non-conventional recursion schemes
(or, induction principles) that always terminate.
These non-conventional recursion schemes are known as
\emph{the Mendler style}.\footnote{We will introduce
	the concept of conventional and non-conventional recursion schemes
	in \S\ref{sec:intro:concepts}.}
Another major design choice in Nax is supporting \emph{term indices} in types,
a middle ground, which sits between polymorphic types and dependent types.

In the following section, we clarify what we mean by the sweet spot between
programming languages and reasoning systems.
%% Our thesis is that
%% the design choices we explain below are reasonable for achieving
%% the goal of combining programming languages and reasoning systems.

\section{Thesis}\label{sec:intro:thesis}
We characterize the sweet spot of the language design for unifying
programming and reasoning by the four necessary features:
%% Whatever design choices we make, the sweet spot should have
%% the following features.
\begin{enumerate}[(1)]
 \item \textbf{A convenient programming} style
         supported by the major constructs of
         modern functional programming languages: 
         parametric polymorphism, recursive datatypes,
         recursive functions, and type inference,

 \item \textbf{An expressive logic} that can specify
	 fine-grained program properties using types,
	 and terms that witness proofs of these properties 
         (the Curry--Howard correspondence),

 \item \textbf{A small theory} based upon a minimal foundational calculus
	 that is expressive enough to support the programming features,
	 expressive enough to embed propositions and proofs about programs,
	 and logically consistent to avoid paradoxical proofs in the logic, and

 \item \textbf{A simple implementation} that keeps the trusted base small.
\end{enumerate}
Our thesis is
%% We claim 
that a language design based on \emph{Mendler-style recursion schemes}
and \emph{term-indexed types} can lead to a system that supports these four
features.

\paragraph{}
From a bird's-eye view, the following chapters back up our claim as follows:
Mendler-style recursion schemes support (1) because they are based on
parametric polymorphism and well-defined over a wide range of datatypes.
Term-indexed types support (2), because they can statically track
program properties. For instance the size of data structures can be tracked
by using a natural number term in their types.
To support (3), we design several foundational calculi, each which extends
a well known polymorphic lambda calculus with term-indexed types.
Mendler-style recursion schemes also also support (4) because their
termination is type-based -- no need for an auxiliary termination checker.

In next section, we summarize the two important concepts mentioned
in our thesis: Mendler-style recursion schemes and term-indexed types.

\section{Mendler-style recursion and term-indexed types}
\label{sec:intro:concepts}
We give summaries of the following preliminary concepts:
Mendler-style recursion schemes (\S\ref{sec:intro:concepts:mendler}),
and term-indexed types (\S\ref{sec:intro:concepts:indexed}).
Further details and historical backgrounds on each of these concepts
will appear in the following chapters (see \S\ref{sec:intro:overview}
for the overview of chapter organization).

%% \subsection{The Curry--Howard correspondence and Normalization}
%% \label{sec:intro:concepts:CH}
%% One promising approach to designing a system that unifies
%% logical reasoning and programming is \emph{the Curry--Howard correspondence}.
%% Howard \cite{Howard69} observed that a typed model of computation
%% (\ie, a typed lambda calculus) gives an interpretation to a (natural deduction)
%% proof system (for an intuitionistic logic).
%% 
%% The Curry--Howard correspondence is a promising approach to designing a
%% unified system for both logical reasoning and programming. Only one language
%% system is needed for both the logic and the programming language. An
%% alternate approach is to use an external logical language to talk about
%% programs as the objects that the logic reasons about. In this approach, one
%% has the obligation to argue that the soundness of the logic, with respect to
%% the programming language semantics, holds.
%% 
%% Under the Curry--Howard correspondence, the logic is internally related to
%% the semantics of program -- there is no need to externally argue for
%% the soundness of the logic with respect to the programming language.
%% The soundness of the logic follows directly from the type preservation of
%% the language under the Curry--Howard correspondence.
%% 
%% Let us consider a proposition to be true (or, valid) when it has a normal proof.
%% That is, there is a program, whose type is the proposition under consideration,
%% and that program has a normal form. By type preservation, any term, of
%% that type, will preserve its type during the reduction steps. Thus reduction
%% preserves truthfulness. If we assume that the language is normalizing
%% (\ie, every well-typed term reduces to a normal form), then any term of
%% that type, which is a not in normal form, implies the existence of
%% a normal proof, which in turn implies that the proposition specified by
%% the type is indeed true. That is, all provable propositions are valid
%% (\ie, the logic is sound) when the language is \emph{type preserving}
%% and \emph{normalizing}.
%% 
%% Therefore, useful logical reasoning systems based on the Curry--Howard
%% correspondence (\eg, Coq, Agda) never support language features that can
%% lead to diverging terms. For example, in both Coq and Agda,
%% unrestricted general recursion (at term level) is not supported. 
%% Instead, these logical reasoning systems
%% often provide principled recursion schemes over recursive types that are
%% guaranteed to normalize. 
%% 
%% Recursive types (\ie, recursion at type level)
%% can also lead to diverging terms when they are not restricted carefully.
%% Many of the conventional logical reasoning systems, based on
%% Curry--Howard correspondence, restrict recursive types in a way,
%% which is not an ideal design choice, if one's goal is a unified system for
%% logic and programming. Our approach explores another design space not yet
%% completely explored. We introduce both approaches to restricting recursive
%% types to ensure normalization in the following two subsections.

\subsection{Restriction on recursive types for normalization}
\label{sec:intro:concpets:recursive}
We have discussed that logical reasoning systems establish
the Curry--Howard correspondence assuming normalization.
So, one challenge to the successful design of reasoning systems
is how to restrict recursion so that all well-typed terms have normal forms.
There are two different design choices illustrated
in Figure~\ref{fig:approaches}, in contrast to
the unrestricted general recursion in functional languages.
The conventional approach restricts the formation of recursive types
(\ie, the restriction is in datatype definition), and
the Mendler-style approach restricts the elimination of the values of
recursive types (\ie, the restriction is in pattern matching).

\begin{figure}
{\centering
\begin{tabular}{p{3cm}|p{12.5cm}}
\parbox{3cm}{Unrestricted\\$~~$general\\$~$recursion\\in functional\\$~~$languages} &
\parbox{12.5cm}{
 kinding:~
  \inference[($\mu$-form)]{\Gamma |- F : * -> *}{\Gamma |- \mu F : *} \\
 \\
 typing:\quad
  \inference[($\mu$-intro)]{\Gamma |- t : F (\mu F)}{\Gamma |- \In~t:\mu F} ~~~~
  \inference[($\mu$-elim)]{\Gamma |- t : \mu F}{\Gamma |- \unIn~t : F (\mu F)}\\
 \\
 reduction:
  \inference[(\unIn-\In)]{}{\unIn~(\In~t) \rightsquigarrow t}
} \\
\\ \hline\hline
\parbox{3cm}{$~$A conventional\\recursion scheme} &
\parbox{12.5cm}{$\phantom{a}$\\
 kinding:~
  \inference[($\mu$-form$^{+}$)]{ \Gamma |- F : * -> * 
                           & \mathop{\mathsf{positive}}(F)}
                           {\Gamma |- \mu F : *} \\
 \\
 typing:~
  \text{{\small($\mu$-intro)} and {\small($\mu$-elim)}
                same as functional language} \\
  \[\inference[(\It)]{\Gamma |- t : \mu F & \Gamma |- \varphi : F A -> A}
                     {\Gamma |- \It~\varphi~t : A}\]
 reduction:~ \text{{\small(\unIn-\In)} same as functional language}
  \[\inference[(\It-\In)]{}{\It~\varphi~(\In~t) \rightsquigarrow
                            \varphi~(\textsf{map}_F~(\It~\varphi)~t)}\]
}
\\ \hline
\parbox{3cm}{A Mendler-style\\recursion scheme} &
\parbox{12.5cm}{$\phantom{a}$\\
 kinding:~ \text{{\small($\mu$-form)} same as functional language} \\
 \\
 typing:~
  \text{{\small($\mu$-intro)} same as functional language}
  \[\inference[(\MIt)]
     { \Gamma |- t : \mu F &
       \Gamma |- \varphi : \forall X . (X -> A) -> F X -> A}
     {\Gamma |- \MIt~\varphi~t : A} \]
 reduction:~
  \inference[(\MIt-\In)]
     {}
     {\MIt~\varphi~(\In~t) \rightsquigarrow \varphi~(\MIt~\varphi)~t}
}
\end{tabular} }
\caption{Two different approaches to terminating recursion schemes
         (in contrast to unrestricted general recursion in functional languages)}
\label{fig:approaches}
\end{figure}

\paragraph{Recursive types in functional programming languages.}
Let us start with a review of the theory of recursive types used
in functional programming languages.
%% Since functional languages
%% are not expected to be normalizing, restrictions are few.

Just as we can capture the essence of unrestricted general recursion at term
level, by a fixpoint operator (usually denoted by \textsf{Y} or \textbf{fix}),
we can capture the essence of recursive types by the
use of a recursive type operator, $\mu$, at type level. 
The rules for the formation {\small($\mu$-form)},
introduction {\small($\mu$-intro)}, and elimination {\small($\mu$-elim)} of
the recursive type operator $\mu$ are described in Figure \ref{fig:approaches}.
We also need a reduction rule {\small(\unIn-\In)}, which relates \In,
the data constructor for recursive types, and \unIn, the destructor for
recursive types, at the term level.

The recursive type operator, $\mu$, as described in Figure \ref{fig:approaches},
is already powerful enough to express non-terminating programs, even without
introducing the general recursive {\em term} operator, \textbf{fix},
to the language. We illustrate this below.\footnote{This is essentially
	the same example we discussed in \S\ref{ssec:intro:scope:datarec},
	but this time using $\mu$.}
First, here is a short reminder of how a fixpoint at the term level operates.
The typing rule and the reduction rule for \textbf{fix} can be given as follows:
\[ \text{typing:}~ \inference{\Gamma |- f : A -> A}{\textbf{fix}\,f : A}
 \qquad\qquad
   \text{reduction}:~ \textbf{fix}\,f \rightsquigarrow f(\textbf{fix}\,f)
\]
We can actually implement \textbf{fix}, using $\mu$, as follows
(using Haskell-like syntax):
\begin{align*}
& \textbf{data}~T\;a\;r = C\;(r -> a) \quad
          \texttt{-}\texttt{-}~\text{\small a non-recursive datatype} \\
& w \,:\, \mu(T\;a) -> a ~~ \quad
          \texttt{-}\texttt{-}~\text{\small an encoding of the untyped
                                     $(\lambda x.x\;x)$
                                     in a typed language}~ \\
& w = \lambda x . \,\textbf{case}~\unIn~x~\textbf{of}~C\;f -> f\;x \\
& \textbf{fix} \,:\, (a -> a) -> a \quad
          \texttt{-}\texttt{-}~\text{\small an encoding of 
                                     $(\lambda f.(\lambda x.f(x\;x))\,
                                                 (\lambda x.f(x\;x)))$} \\
& \textbf{fix} = \lambda f. (\lambda x. f (w\;x))\,(\In(C(\lambda x. f (w\;x))))
\end{align*}

Thus, to avoid the loss of termination guarantees, we need to alter the rules
for $\mu$, in someways, to ensure a consistent logic. One way, is to restrict
the rule {\small $\mu$-form}; the other way, is to restrict the rule
{\small $\mu$-elim}. Once we decide which of these two alterations of the
rules we will use, the design of principled recursion combinators (\eg, \It\
for the former and \MIt\ for the latter) follows from that choice.

\paragraph{Positive (recursive) datatypes and negative (recursive) datatypes.}
Positive datatypes have recursion only in covariant positions.
For example, $\mu\,T_2$, where $\textbf{data}\;T_2 = C_2\,(Bool -> r)$,
is a positive datatype since the recursive argument $r$ in
the base structure $T_2$ only appears in the covariant position.
Recursive datatypes that have no function arguments are by default
positive datatypes. For instance, the natural number datatype $\mu\,N$,
where $\textbf{data}\,N\,r=S\,r \mid Z$, is a positive datatype.


Negative datatypes have recursion in contravariant positions.
Note that $\mu(T\;a)$ in the example above is a negative datatype
since the recursive argument $r$ in the base structure $T$ appears
in the contravariant position. Another example of a negative datatype is
$\mu\,T'$, where $\textbf{data}\;T'\;r = C'\,(r -> r)$, since $r$ in $T'$
appears in both contravariant and covariant positions.

\paragraph{Recursive types in the conventional approach}
In the conventional approach, the formation (\ie, datatype definition) of
recursive types is restricted, but arbitrary elimination (\ie, pattern matching)
over the values of recursive types is allowed. In particular, the formation of
negative recursive types is restricted. Only positive recursive types are
supported. Thus, in Figure \ref{fig:approaches}, we have a restricted version of
the formation rule {\small($\mu$-form$^{+}$)} has an additional condition that
require $F$ to be positive. The other rules {\small($\mu$-intro)},
{\small($\mu$-elim)}, and {\small(\unIn-\In)} remain the same as for
functional languages. Since we have restricted the recursive types
at the type level and we do not have general recursion at the term level,
the language is indeed normalizing. However, we can neither write
interesting (\ie, recursive) programs that involves recursive types nor
inductively reason about those programs, unless we have principled recursion
schemes that are guaranteed to normalize. One such recursion scheme is called
iteration (\aka\ catamorphism). The typing rules for the conventional iteration
\It\ are illustrated in Figure \ref{fig:approaches}. Note, we have the typing
rule {\small(\It)} and the reduction rule {\small(\It-\In)} for \It\,
in addition to the rules for the recursive type operator $\mu$.

\paragraph{Recursive types in the Mendler-style approach}
In the Mendler-style approach, we allow arbitrary formation
(\ie, datatype definition) of recursive types, but we restrict
the elimination (\ie, pattern matching) over the values of recursive types. 
The formation rule {\small($\mu$-form)} remains the same as
for functional languages. That is, we can define arbitrary recursive types,
both positive and negative. However, we no longer have the elimination
rule {\small($\mu$-elim)}. That is, we are not allowed to pattern match over
the values of recursive types in the normal fashion. We can only pattern match
over the values of recursive types through the Mendler-style recursion
combinators. The rules for the Mendler-style iteration combinator \MIt\
are illustrated in Figure \ref{fig:approaches}.
Note, there are no rules for \unIn\ in the Mendler-style approach.
The typing rule {\small($\mu$-elim)} is replaced by {\small(\MIt)} and
the reduction rule {\small(\unIn-\In)} is replaced by {\small(\MIt-\In)}.
More precisely, the typing rule {\small \MIt} is both an elimination rule
for recursive types and a typing rule for the Mendler-style iterator.
You can think of the rule {\small(\MIt)} as replacing both the elimination rule
{\small($\mu$-elim)} and the typing rule for conventional iteration
{\small(\It)}, but in a safe way that guarantees normalization.

\subsection{Justification of the Mendler-style as a design choice.}
\label{sec:intro:concepts:mendler}
We choose to base our approach to the design of a seamless synthesis of both
logic and programming on the Mendler-style. It restricts the elimination (\ie,
pattern matching) over values of recursive types, rather restricting the
formation (\ie, datatype definition) of recursive types (a more conventional
approach). The impact of this design choice is that it enables the logic to
include all datatype definitions that are used in functional programming
languages.

Functional programming promotes ``functions as first class values''.
It is natural to pass functions as arguments and embed functions into
(recursive) datatypes. If embedding functions in datatypes is allowed,
we can embed a function whose domain is the very type we are defining.
For example, the recursive datatype definition
$\mathbf{data}~T = C\;(T -> \textit{A})$ in Haskell is such a recursive
datatype definition. Such datatypes are called negative recursive datatypes
since the recursive occurrence $T$ appears in a negative position.
We say that $T$ is in a negative position, since $(T -> A)$ is analogous to
$(\neg T \land A)$ when we think of $->$ as a logical implication. There exist
both interesting and useful examples in functional programming involving
negative datatypes. In \S\ref{sec:msf}, we illustrate that
the Mendler-style recursion scheme we discovered can express
interesting examples involving negative datatypes.

Recall that the motivation of this dissertation research
(quoting again from the beginning of this chapter)
is to contribute to answering the question of
``how does one build a seamless system where programmers can both
write (functional) programs and formally reason about those programs''.
Under the Curry--Howard correspondence,
to formally reason about a program, the logic needs to refer to the type of
the program, since the type, interpreted as a proposition, describes
the property of the program. Since the Mendler-style approach does not
restrict recursive datatype definitions, we can directly refer to the types
of programs that use negative recursive datatypes.

The Mendler style is a promising approach to building a unified system because
all the recursive types (both positive and negative) are definable and
the recursion schemes over those types are normalizing.
Although the conventional approach is widely followed
in the design of formal reasoning systems (\eg, Coq, Agda), it cannot directly
refer to programs that use non-positive recursive types. One may object that
it is possible to indirectly model negative recursive types
in the conventional style, via alternative equivalent encodings
which map negative recursive types into positive ones. But, such
encodings do not align with our motivation towards a seamless unified
system for both programming and reasoning. It is undesirable to require
programmers to significantly change their programs just to reason about them.
If the change is unavoidable, it should be kept small. That is,
the changed program should syntactically resemble the original program,
which the programmer would usually write in a functional programming language.
In Chapter 3, we show a number of examples of programs written in
the Mendler style that look more close to the programs written using
general recursion than the programs written in the conventional style.

\subsection{Term-indexed types, type inference, and datatypes}
\label{sec:intro:concepts:indexed}
One of the most frequently asked questions about our design choices for Nax,
regarding term-indexed types, is ``why not dependent types?". Our answer
is that a moderate extension to the polymorphic calculus is a better candidate
than a dependently typed calculus as the basis for a practical programming
system. Recall, that we hope to design a unified system for programming
as well as reasoning. Language designs based on indexed types can
benefit from existing compiler technology and type inference algorithms
for functional programming languages. In addition, theories for
term-indexed datatypes are simpler than theories for full-fledged
dependent datatypes, because term-indexed datatypes can be encoded as
functions (using Church-like encodings).

The implementation technology for functional programming languages based on
polymorphic calculi is quite mature. There exist industrial
strength implementations, such as the Glasgow Haskell Compiler (GHC),
whose intermediate core language is an extension of \Fw.
Our term-indexed calculi described in Part \ref{part:Calculi} are closely
related to \Fw\ by an index-erasure property. The hope is that
our implementation can benefit from these technologies.

Type inference algorithms for functional programming languages are often
based on certain restrictions of the Curry-style polymorphic lambda calculi.
These restrictions are designed to avoid higher-order unification during
type inference.
We develop a conservative extension of Hindley--Milner type inference for
Nax (Chapter \ref{ch:naxTyInfer}). This is possible because Nax is based on our
term-indexed calculi (Part \ref{part:Calculi}). Dependently typed languages,
on the other hand, are often based on bidirectional type checking, which
requires annotations on top level definitions, rather than
Hindley--Milner-style type inference.

In dependent type theories, datatypes are usually supported as primitive
constructs with axioms, rather than as functional encodings
(\eg, Church encodings). One can give functional encodings for datatypes
in a dependent type theory, but one soon realizes that the induction principles
(or, dependent eliminators) for those datatypes cannot be derived within
the pure dependent calculi \cite{Geuvers01}.
So, dependently typed reasoning systems support datatypes as primitives.
For instance, Coq is based on Calculus of Inductive Constructions, which
extends Calculus of Constructions \cite{CoqHue86} with dependent datatypes
and their induction principles.

In contrast, in polymorphic type theories, all imaginable datatypes
within the calculi have functional encodings (\eg, Church encodings).
For instance, \Fw\ need not introduce datatypes as primitive constructs,
since \Fw\ can embed all imaginable datatypes, including non-regular
recursive datatypes with type indices. 

Another reason to use term-indexed calculi, rather than dependent type theories,
is to extend the application of Mendler-style recursion schemes,
which are well-understood in the context of \Fw.
Researchers have thought about (though not published)\footnote{
     Tarmo Uustalu described this on a whiteboard
     when we met with him at the University of Cambridge in 2011.
     We discuss this in the related work chapter (\S\ref{sec:relwork:dep}).}
Mendler-style primitive recursion over dependently-typed functions
over positive datatypes (\ie, datatypes that have a map), but not for
negative (or, mixed-variant) datatypes. In our term-indexed calculi,
we can embed Mendler-style recursion schemes (just as we embedded them in \Fw)
that are also well-defined for negative datatypes.

\section{Contributions}\label{sec:intro:contrib}
This dissertation makes contributions in several areas.
\begin{itemize}
\item[1.]
    It organizes and expands the realm of \emph{Mendler-style recursion schemes}
    (Part~\ref{part:Mendler})

\item[2.] It establishes a meta-theories for \emph{term-indexed types}
        (Part~\ref{part:Calculi}),

\item[3.] It designs a practical language (with an implementation)
        \emph{in the sweet spot} between programming and logical reasoning
        (Part~\ref{part:Nax}), and

\item[4.] It identifies several interesting open problems related to above.
\end{itemize}

\subsection{Contributions related to the Mendler style}
We organize a hierarchy of Mendler-style recursion schemes in two dimensions.
The first dimension is the abstract operations they support. For instance,
the Mendler-style iteration (\MIt) supports a single abstract operation
the recursive call. All the other Mendler-style recursion schemes
support the recursive call and an additional set of abstract operations. 
The second dimension is over the kind of the datatypes they operate over.
For example, \texttt{Nat} has kind $*$, while \texttt{Vec}
has kind $* -> \mathtt{Nat} -> *$. Each recursion scheme is actually a
family of recursion combinators sharing the same term definition
(\ie, uniformly defined) but with different type signatures at each kind.

We expand the realm of Mendler-style recursion schemes in several ways.
First, we report on a new recursion scheme $\MsfIt$, which is useful
for negative datatypes.  Second, we study the termination behaviors
of Mendler-style recursion schemes. Some recursion schemes (\eg, \MIt, \MsfIt)
always terminate for any recursive type, while others (\eg, \McvPr) only
terminate for certain classes of recursive types. Third, we extend
all Mendler-style recursion schemes to be expressive over term-indexed types.
The Mendler style has been studied in the context of \Fw\ (and several
extensions) which can express \emph{type}-indexed types. To extend Mendler-style
recursion schemes to be expressive over \emph{term}-indexed types, we report on
several theories for calculi (\Fi\ and \Fixi) that support term indices.
This is another important area of our contribution.

We provide examples that illustrate when each recursion scheme is useful
in Chapter \ref{ch:mendler}. The most interesting example among them is
the type-preserving evaluator for a simply-typed Higher-Order Abstract Syntax
(HOAS) (\S\ref{sec:evalHOAS}), which involves negative datatypes with indices.
This example is our novel discovery, which reports that
a type-preserving evaluator for a simply-typed HOAS can be expressed within \Fw.

In addition, we develop a better understanding of some existing
Mendler-style recursion schemes. For instance, the existence of
Mendler-style course-of-values recursion (\McvPr) is reported
in the literature, but the calculus that can embed \McvPr\ was unknown.
We embed Mendler-style course-of-values recursion into \Fixi\ 
(or into \Fixw\ \cite{AbeMat04}, when we do not consider term-indices).

\subsection{Contributions to the theory of Term-Indexed Types}
Mendler-style recursion schemes have been studies in the context of
polymorphic lambda calculi. For instance, \citet{AbeMatUus03} embedded 
Mendler-style iteration (\MIt) into \Fw\ and \citet{AbeMat04} embedded
Mendler-style primitive recursion (\MPr) into \Fixw. These calculi
support type-indexed types.

To extend the realm of Mendler-style recursion schemes to include
term-indexed types, we extended \Fw\ and \Fixw\ to support term indices.
In Part~\ref{part:Calculi}, we present our new calculi
\Fi\ (Chapter~\ref{ch:fi}), which extends \Fw\ with term indices, and
\Fixi\ (Chapter~\ref{ch:fixi}), which extends \Fixw\ with term indices.
These calculi have an erasure property that states that well-typed terms
in each calculus are also well typed terms (when erased) in the 
underlying calculus. For instance, any well typed term in \Fi\ is also
a well-typed term in \Fw, and there are no additional well-typed terms
in \Fi\ that are not well-typed in \Fw.

Our new calculi, \Fi\ and \Fixi, are strongly normalizing and
logically consistent. We show strong normalization and logical consistency
using the erasure properties. That is, strong normalization and
logical consistency of \Fi\ and \Fixi\ are inherited from \Fw\ and \Fixw.
Since \Fi\ and \Fixi\ are strong normalizing and logically  consistent,
the Mendler-style recursion schemes that can be embedded into these calculi
are adequate for logical reasoning as well as programming.

\subsection{Contributions in the design of the Nax language}
We design and implement a prototypical language Nax that explores
the sweet spot between programming oriented systems and logic oriented systems.
The language features supported by Nax provide the advantages
of both programming oriented systems and logic oriented systems.
Nax supports both term- and type-indexed datatypes,
rich families of Mendler-style recursion combinators,
and a conservative extension of Hindley--Milner type inference.
We designed Nax so that its foundational theory and
implementation framework could be kept simple.

Term- and type-indexed datatypes can express fine grained program properties
via the Curry--Howard correspondence, as in logic oriented systems. Although
not as flexible as full-fledged dependent types, indexed datatypes can
still express program invariants, such as stack-safe compilation
(\S\ref{sec:example}), and size invariants on data structures.
Index types can simulate much of what
dependent types can do using singleton types. Since Nax has only erasable
indices, the foundational theory can be kept simple, and it supports
features that have the advantages of programming oriented systems 
(\eg, type inference, arbitrary recursive datatypes).

Adopting Mendler style provides merits of both programming oriented systems
and logic oriented systems. Since Mendler style is elimination based, one can
define all recursive datatypes usually supported in functional programming
languages. In addition, the programs written using Mendler-style recursion
combinators look more similar to the programs written using general recursion
than programs written in Squiggol style.
Since Nax supports only the well-behaved (\ie, strongly normalizing)
Mendler-style recursion combinators, it is safe to construct proofs using them.
In addition, Mendler-style recursion combinators are naturally well-defined
over indexed datatypes, which are essential to express fine-grained program
properties. Mendler style provides type based termination, that is, termination
is a by-product of type checking. Thus, it makes the implementation framework
simple since we do not need extra termination checking theories or algorithm.

Hindley--Milner-style type inference is familiar 
to functional programmers.
Nax can infer types for all programs that involve only regular datatypes,
which are already inferable in Hindley--Milner, without any type annotation.
Nax requires programs involving indexed datatypes to annotate their eliminators
by index transformers, which specify the relation between the input type index
and the result type. Eliminators of non-recursive datatypes are case expressions
and eliminators of recursive datatypes are Mendler-style recursion combinators.

\subsection{Contributions identifying open problems}
We identify several open problems alongside the contributions mentioned
in previews subsections. We will discuss the details of these open problems
in the future work chapter (Chapter \ref{ch:futwork}).
Here, we briefly introduce two of them.

\paragraph{Handling different interpretations of $\mu$ in one language system:}
Nax provides multiple recursion schemes (or, induction principles) used
to describe different kinds of recursive computations over recursive datatypes.
These recursion schemes are all motivated by concrete examples, which explains
the need for multiple schemes. It is more convenient to express various kinds of
recursive computations in Nax, by choosing a recursion scheme that fits
the structure of the computation, than in those systems that provide
only one induction scheme. However, there is theoretical difficulty
handling multiple interpretations of the recursive type operator $\mu$
in one language system.

Recall that we can embed datatypes as functional encodings in
our indexed type theory. Recursive datatypes and their recursion schemes in Nax
are embedded using Mendler-style encodings.
In Mendler style, one encodes the recursive type operator $\mu$
and its eliminator (the recursion scheme) as a pair.
So, there are several different encodings of $\mu$,
one for each recursion scheme. Some recursion schemes subsume others
(\ie, the more expressive one can simulate the other).

It would have been easy to describe the theory for Nax if we had
one most powerful recursion scheme that subsumes all the others,
which leads to a single interpretation of $\mu$. Unfortunately, we know of
no Mendler-style recursion scheme that subsumes all the other recursion schemes
in Nax. For instance, iteration (\MIt) can be subsumed by either 
iteration with a syntactic inverse (\MsfIt) or primitive recursion (\MPr).
But, there is no known recursion scheme that can subsume both \MsfIt\ and \MPr.

However, we strongly believe that it is okay to apply \MsfIt\ to
the result of \MPr\ (when \MPr\ produces a recursive value) and vice versa.
Intuitively, the different interpretations of $\mu$ only matter during
the internal computation of the recursion scheme. That is, one may consider
that (recursive) values resulting from different recursion schemes
share a common abstract representation of $\mu$.
The theoretical justification for this is still ongoing work.

\paragraph{Deriving positivity (or monotonicity) from polarized kinds:}
One can extend the kind syntax of arrow kinds in \Fw\ with polarities
($p\kappa_1 -> \kappa_2$ where the polarity $p$ is either $+$, $-$, or $0$)
to track whether a type constructor argument is used in
covariant (positive), contravariant (negative), or
mixed-variant (both positive and negative) positions.
It is still an open problem whether it is possible to derive monotonicity
(\ie, the  existence of a map) for a type constructor from its polarized kind,
without examining the type constructor definition.

We identified a useful application for a solution to this open problem.
We discovered an embedding of Mendler-style course-of-values recursion in
a polarized system for positive (or monotone) type constructors.
That is, once you can show the existence of a map for a datatype,
course-of-values recursion always terminates.
However, in a practical language system, it is not desirable to burden users
with the manual derivation for every datatype on which they might want to
perform course-of-values recursion. If the type system can automatically
categorize datatypes that have maps from their polarized kinds,
this burden can be alleviated.

\newpage{}
\section{Methodology and Overview}\label{sec:intro:overview}
\begin{figure}
\input{intro_figoverview}
\caption{Summary on how key concepts are related}
\label{fig:overview}
\end{figure}

This dissertation consist of five parts:
\begin{itemize}
\item Part \ref{part:Prelude} (Prelude),
\item Part \ref{part:Mendler} (The Mendler style),
\item Part \ref{part:Calculi} (Term-indexed lambda calculi),
\item Part \ref{part:Nax} (The Nax language), and
\item Part \ref{part:Postlude} (Postlude).
\end{itemize}
The three parts in the middle describe the three steps of our approach. 
First, we explore new ideas about
Mendler-style recursion schemes driven from concrete examples
using Haskell (with some GHC extensions). Second, we develop
theories (\ie, lambda calculi) for term-indexed datatypes to prove that
the Mendler-style recursion schemes are well-defined over indexed datatypes
and have the expected termination behavior. Third, we design a language system
with practical features that implements our ideas and is based on the
theory we have developed. Figure~\ref{fig:overview} summarizes the organization of
key concepts throughout the dissertation.

\paragraph{Part \ref{part:Prelude} (Prelude)}\hspace{-1em} 
comprises Chapter \ref{ch:intro} (which you are currently reading)
and Chapter \ref{ch:poly} which 
reviews the theory of several well-known typed lambda calculi:
the simply-typed lambda calculus (STLC) (\S\ref{sec:stlc}),
System \F\ (\S\ref{sec:f}),
System \Fw\ (\S\ref{sec:fw}), and
the Hindley--Milner type system (\S\ref{sec:hm}).

In Sections \ref{sec:stlc}-\ref{sec:fw}, we review strong normalization proofs
(using saturated sets) for each of the three calculi:
STLC (no polymorphism), System \F\ (polymorphism over types), and
System \Fw\ (polymorphism over type constructors).

The later proofs each extend the normalization proof of the previous calculus.
We will use the strong normalization of System \Fw\ to show that
our term-indexed lambda calculi in Part \ref{part:Calculi} are
strongly normalizing. Readers familiar with strong normalization proofs
of these calculi may skip or quickly skim over these sections.
It is worth noticing two stylistic choices in our formalization of
System \F\ and \Fw: (1) terms are in Curry style and
(2) typing contexts are divided into two parts
    (one for type variables and the other for term variables).
This choice prepares readers for our formalization of the term-indexed calculi
in Part \ref{part:Calculi}, which have Curry-style terms and
typing contexts divided into two parts.

In \S\ref{sec:hm}, we review the type inference algorithm for
the Hindley--Milner type system (\S\ref{sec:hm}).
The Hindley--Milner type system (HM) is a restriction of System~\F,
which makes it possible to infer types without any type annotation on terms.
Later in Part~\ref{part:Nax} Chapter \ref{ch:naxTyInfer},
we formulate a conservative extension of HM, which restricts
the term-indexed calculus System \Fi\ (Chapter \ref{ch:fi}) in a similar manner.

\paragraph{Part \ref{part:Mendler} (the Mendler style)}\hspace{-1em} introduces
the concept of Mendler-style recursion schemes (Chapter \ref{ch:mendler})
using examples written in Haskell (with some GHC extensions). The readers
of Chapter \ref{ch:mendler} need no background knowledge on typed lambda calculi
but only some familiarity with functional programming. We explain the concepts of
a number of Mendler-style recursion schemes, their termination properties, and
how one recursion scheme is related to another. 
We also provide semi-formal proofs of termination
for some of the recursion schemes (\MIt\ and \MsfIt) by embedding them
into the \Fw\ fragment of Haskell. More formal and general
proofs, by embedding the schemes into our term-indexed lambda calculi, come later in
Part~\ref{part:Calculi}.

The Mendler-style recursion schemes discussed in Chapter \ref{ch:mendler}
include iteration (\MIt), iteration with syntactic inverse (\MsfIt),
primitive recursion (\MPr), course-of-values iteration (\McvIt),
and course-of-values recursion (\McvPr). Of these, \MsfIt\ was discovered
while writing this dissertation.
There are even more Mendler-style recursion schemes, which are not
discussed in Chapter \ref{ch:mendler} -- we give pointers to them in our
related work chapter (Chapter \ref{ch:relwork} of Part \ref{part:Postlude}).

\paragraph{Part \ref{part:Calculi} (term-indexed lambda calculi)}\hspace{-1em}
develops theories for term-indexed types.
We formalize two term-indexed lambda calculi,
System \Fi\ (Chapter \ref{ch:fi}) and System \Fixi\ (Chapter \ref{ch:fixi}),
which are extensions of polymorphic calculi.
System \Fi\ extends System \Fw\ with term indices and
System \Fixi\ extends System \Fixw\ \cite{AbeMat04} with term indices.

We prove both strong normalization and logical consistency of
these term-indexed calculi using an index erasure property.
The index erasure property of a term-indexed calculus
projects a typing in the term-index calculi into
the polymorphic calculus that the term-indexed calculus extends.
That is, all well-typed terms in \Fi\ and \Fixi\ are
also well-typed typed terms in \Fw\ and \Fixw.


By embedding Mendler-style  recursion schemes into our term-indexed lambda calculi,
we prove that those schemes are well-defined and
terminate over term-indexed datatypes.  For instance,
\MIt\ and \MsfIt\ can be embedded into System \Fi,
and, \MPr\ and \McvPr\ can be embedded into System \Fixi.

\paragraph{Part \ref{part:Nax} (the Nax language)}\hspace{-1em} consists of
three chapters.
First, we introduce the features of Nax (Chapter \ref{ch:naxFeatures})
in a tutorial format using small Nax code snippets as examples.
Next, we discuss the design principles of the type system (Chapter \ref{ch:nax})
by comparing it to two other systems: Haskell's datatype promotion and Agda.
In Chapter \ref{ch:nax} we develop
larger and more practical examples,
a type-preserving interpreter and a stack safe compiler.
Lastly, we discuss type inference in Nax (Chapter \ref{ch:naxTyInfer}),
which is a conservative extension of the Hindley--Milner type system (HM).
That is, any program whose type is inferable in HM, can also have its type
inferred in Nax without any annotation. Programs involving
term- or type-indexed datatypes, which are not supported in HM, need
some annotation to infer their types in Nax. These annotations are only
required on three syntactic entities (datatype declarations, case expressions,
and Mendler-style recursion combinators).

\paragraph{Part \ref{part:Postlude} (Postlude)}\hspace{-1em} closes
the dissertation by summarizing
  related work (Chapter~\ref{ch:relwork}),
  future work (Chapter~\ref{ch:futwork}), and
  conclusions (Chapter~\ref{ch:concl}).

