\section{Introduction}
During the past decade, the functional programming community achieved
partial success in their goal of maintaining fine-grained properties
by moderate extensions to the type system of functional languages\cite{CheHin03,CheHin02,Xi03}.
This approach is often called \emph{``lightweight''}
(\eg, lightweight dependent types\footnote{
  \url{http://okmij.org/ftp/Computation/lightweight-dependent-typing.html} },
  lightweight program verification),
since using a full blown proof assistant to maintain similar properties is likely to require
much more effort (heavyweight).

The Generalized Algebraic Data Type (GADT) extension, implemented
in the Glasgow Haskell Compiler (GHC), has made this approach
possible even when performing everyday functional programming tasks.
%
Unfortunately, implementations supporting a lightweight approach (\eg, GHC)
lack \textbf{correctness guarantees} and \textbf{type inference} in general.
In addition, practical implementations often lack support for GADTs indexed
by terms, so \textbf{term indices are faked} (or simulated) by additional
type structure replicating the requisite term structure. We believe that the lightweight approach can
become more productive and reliable if we can resolve these problems.

\paragraph{Problem 1. \textbf{Correctness guarantee}} ~

Proof assistants based on dependent types can
\emph{express fine-grained properties} and
also \emph{guarantee correctness} since these calculi are based on
strongly normalizing and logically consistent systems. For instance, Coq is
based on the Calculus of Inductive Constructions, which is a dependently-typed
$\lambda$-calculi known to be strongly normalizing and logically consistent.
 
But, the same fine-grained properties are not expressible in ordinary functional
programming languages with only simple polymorphic types, since such languages lack the
expressivity of dependent types. Even if these fine-grained properties were somehow
expressible, one would not have any guarantee of correctness. Recall that general purpose
programming languages are neither strongly normalizing nor logically consistent, because
they are (by design) capable of expressing diverging computations. So, the lightweight
approach in conventional functional languages can only raise programmers confidence of
correctness (assuming that the inconsistent fragment of the type system was never used for
reasoning about the desired properties) but cannot guarantee the correctness of the desired
properties as is the norm in proof assistants.

\paragraph{Problem 2. \textbf{Type inference}} ~

Type inference makes type-safe programming pleasant when performomng
everyday programming tasks, since programmers are freed from including tedious type annotations.
Many typed functional programming languages including Haskell 98 and SML
are based on the Hindley-Milner type system (HM), which is not only
\emph{type-safe} but also supports \emph{type inference}.

An essential feature of the lightweight approach is \emph{indexed datatypes},
which are datatypes with \emph{heterogeneous type parameters}
(\aka \emph{indices}) which are made possible by the GADT extension.
Such datatypes, often used in lightweight approaches, are
beyond the capabilities of polymorphic type schemes used in HM. Inclusion of 
just a simple subset of
the indexed datatypes, such as nested datatypes \cite{BirMee98}, already make
type inference undecidable \cite{Henglein93}. More sophisticated
uses add even more complication\cite{Lin2010}.

So, functional language implementations, that support the lightweight approach,
require type annotations on both indexed datatype declarations and on
function definitions that pattern match indexed datatypes, in order to recover
a semblence of type inference.\footnote{
	Type inference aided by type annotation is also called
	partial type inference or type reconstruction.}
Type annotations on datatype declarations are absolutely necessary
when either the result types of their data constructors have indices
or when the argument types of their data constructors have existential indices.
However, it still an open question of
\emph{where and how much type annotations are needed on function definitions}.

\paragraph{Problem 3. \textbf{Faked term indices}} ~

The indexed datatypes can only have static dependencies (\ie, indices must be compleytely
known at type checking time), unlike full-fledged dependent types, as used in proof assistants,
which can depend on both static and dynamic values. Therefore, having term indices does
not imply full-fledged dependent types.

Indexed datatypes can be indexed by either types or terms, or both.
Type representations \cite{CheHin03} (Fig.\,\ref{fig:rep}) used in
datatype generic programming are typical examples of type-indexed datatypes.
The length-indexed list, or \texttt{Vector}, (Fig.\,\ref{fig:vec})
is an example of a term-indexed datatype.
However, the \texttt{Vector} datatype declaration in Fig.\; \ref{fig:vec}
uses faked term indices. These indexes are faked because
rather than use the real term constructors of the natural numbers
(defined by \verb/data Nat = Succ Nat | Zero/)
it uses the uninhabited types \verb|Succ| and \verb|Zero|
to simulate the data constructors of \verb+Nat+. Such faked term indices are 
problematic since
they (1) duplicate code (\ie, operations on \texttt{Nat} must be redefined
at type level) and (2) have less precise semantics than true term indices
(\eg, cannot prevent ill-typed types such as \texttt{Succ Bool}).

Although indexed datatypes with true term indices have been studied\cite{Zenger97},
including term indices in practical functional languages
is not trivial. Allowing arbitrary terms at the type level breaks the decidability
of type checking due to diverging terms. Term indices in types implies that
type equality depends on term equality. And, obviously, term equality will loop
when one of the terms being compared diverges. Undecidability of type checking
can be lifted once we have resolved \textit{Problem 1}, and make sure that
indices are normalizing.

\begin{figure}
\begin{verbatim}
data Rep t where
  R_Int  :: Rep Int
  R_Char :: Rep Char
  R_List :: Rep a -> Rep [a]
  R_Pair :: Rep a -> Rep b -> Rep (a,b)
\end{verbatim}
\caption{A type representation for \texttt{Int}, \texttt{Char},
	\texttt{[]}, and \texttt{(,)} in Haskell with GADTs}
\label{fig:rep}
\end{figure}

\begin{figure}
\begin{verbatim}
data Succ n
data Zero

data Vector a n where
  VCons :: a -> Vector m a -> Vector a (Succ m)
  VNil  :: Zero
\end{verbatim}
\caption{Length indexed list datatype \texttt{Vector}
	in Haskell with GADTs}
\label{fig:vec}
\end{figure}



%% \paragraph{Contribution} ~\\
\paragraph{} ~\\

To resolve these problems, we have designed and implemented a prototype of
the Nax programming language. The current proptotype of Nax is a
strongly normalizing functional language supporting the following features
(which are all illustrated in \S\ref{sec:bg}):
\begin{description}

\item[Two level datatypes.]
Recursive dataypes are introduced in two stages. First a non-recursive
{\em structure} is introduced which abstracts over where recursive
sub-components will appear. Then a {\em fix-point} is taken to define
the recursive types (\S \ref{2level}). To minimize the extra notation necessary to program
in this manner an extensive {\em macro-facility} is provided. The most common
macro forms can be automatically derived. This is illustrated in \S\ref{macro}.

\item[Indexed types with static term indices.]
A type constructor is applied to arguments. Arguments
are either parameters or indices. A datatype is polymorphic over
its parameters (in the sense that parametricity theorems hold over parameters).
Parameters are always types.
Indexed arguments can be either types or terms. An index usually
encodes a static property about the shape or form of a value with
that type. We use different kinding rules to separate
term indices from type indices. For instance a length indexed list, \verb+x+,
might have type (\verb+List Int 2+). The \verb+Int+ is a parameter, indicating
the list contains integers, but the \verb+2+ is an index indicating
that the list, \verb+x+, has exactly two elements.
Types are static in Nax. Types are only used for type checking
and are computationally irrelevant, even though some parts of a type might include terms.
In other words, Nax supports \emph{index erasure},

\item[Recursive types of unrestricted polarity but restricted elimination.]
It is well known that unrestricted recursive types enable diverging computation
even without any recursion at the term level. To design a normalizing language
that supports recursive types, we must make a design choice that limits
the use of recursive types. There are two possible
design choices. We may restrict the formation of recursive types
(\ie, type definition) or we may restrict the elimination of recursive types
(\ie, pattern matching). In Nax, we make the latter design choice, so that we can
define \emph{all the recursive datatypes} available in modern functional languages.

\item[Mendler style iteration and recursion combinators.] Any useful normalizing
language should support principled recursion operators that guarantee
normalization. Such operators should be easy to use, and expressive over datatypes
with both parameters and indices. Mendler style combinators meet both requirements.
So, we adopt them in Nax.

\item[Type inference (reconstruction) from minimal annotation.]
When we extend the Hindley-Milner type system with indexed data types,
we no longer have type inference for completely unannotated terms.
For example, this restriction shows up in languages which support GADTs,
which support a kind of type indexing.
Although complete
type inference is not possible, partial type inference (reconstruction of missing
type information) is
still possible when sufficient type annotations are provided. Nax's
systematic partition of type parameters from type indices provides
a mechanism where it is possible to decide exactly where
additional type annotations are needed, and to enforce that the
programmer supply such annotations. This system faithfully extends the Hindley-Milner
type inference (\ie, no additional annotations are needed for the programs that are
already inferable by Hindley-Milner).
\end{description}


